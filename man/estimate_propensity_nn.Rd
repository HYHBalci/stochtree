% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate_propensity_nn.R
\name{estimate_propensity_nn}
\alias{estimate_propensity_nn}
\title{Estimate Propensity Scores using a Neural Network}
\usage{
estimate_propensity_nn(
  X_train,
  Z_train,
  X_test = NULL,
  architecture = NULL,
  learning_rate = 0.001,
  epochs = 50,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 0
)
}
\arguments{
\item{X_train}{A numeric matrix or data frame of predictor variables for the training set.}

\item{Z_train}{A numeric vector of binary treatment assignments (0 or 1) for the training set.}

\item{X_test}{(Optional) A numeric matrix or data frame of predictor variables for the test set.
Must have the same columns in the same order as \code{X_train}.}

\item{architecture}{A function that defines the Keras model architecture. The function
must accept one argument, \code{input_dim}, which is the number of input features.
If \code{NULL}, a default flexible architecture is used (see details).}

\item{learning_rate}{The learning rate for the Adam optimizer. Default is 0.001.}

\item{epochs}{Number of training epochs (passes through the entire dataset). Default is 50.}

\item{batch_size}{Number of samples processed before updating model weights. Default is 32.}

\item{validation_split}{Fraction of the training data to be used as validation data for
monitoring early stopping. Default is 0.2 (i.e., 20\%).}

\item{verbose}{Controls training verbosity. 0 = silent, 1 = progress bar. Default is 0.}
}
\value{
A list containing:
\item{train}{A numeric vector of estimated propensity scores for the \code{X_train} data.}
\item{test}{A numeric vector of estimated propensity scores for the \code{X_test} data. Returns \code{NULL} if \code{X_test} is not provided.}
}
\description{
Fits a flexible, feed-forward neural network to model the probability of
treatment assignment (the propensity score), P(Z=1 | X). This serves as a
powerful alternative to traditional models like logistic regression, capable
of capturing complex non-linearities and interactions among covariates.

The function handles data scaling internally and uses early stopping to
prevent overfitting based on validation loss.
}
\details{
This function requires the \code{keras} package and a configured TensorFlow backend.

The default neural network architecture is a sequential model with:
\enumerate{
\item A dense layer with 128 units and ReLU activation.
\item A batch normalization layer.
\item A dropout layer with a rate of 0.3.
\item A dense layer with 64 units and ReLU activation.
\item A batch normalization layer.
\item A dropout layer with a rate of 0.2.
\item A final dense output layer with 1 unit and a sigmoid activation for probability output.
}

The early stopping callback monitors \code{val_loss} and restores the best model weights.
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
n <- 500
p <- 5
X <- matrix(rnorm(n * p), ncol = p)
true_propensity <- pnorm(-0.5 + X[,1] + 0.5 * X[,2]^2)
Z <- rbinom(n, 1, true_propensity)

# Split data
train_idx <- sample(1:n, 400)
X_train <- X[train_idx, ]
Z_train <- Z[train_idx]
X_test <- X[-train_idx, ]

# Estimate propensity scores using the NN with explicit parameters
nn_scores <- estimate_propensity_nn(
  X_train = X_train,
  Z_train = Z_train,
  X_test = X_test,
  epochs = 40,
  batch_size = 32,
  verbose = 1 # Show progress bar during training
)

# View a summary of the estimated scores
summary(nn_scores$train)
}
}
