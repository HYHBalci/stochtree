{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from stochtree import BARTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Generate covariates\n",
    "n = 1000\n",
    "p_X = 10\n",
    "X = rng.uniform(0, 1, (n, p_X))\n",
    "\n",
    "\n",
    "# Define the outcome mean function\n",
    "def outcome_mean(X):\n",
    "    return np.where(\n",
    "        (X[:, 0] >= 0.0) & (X[:, 0] < 0.25),\n",
    "        -7.5 * X[:, 1],\n",
    "        np.where(\n",
    "            (X[:, 0] >= 0.25) & (X[:, 0] < 0.5),\n",
    "            -2.5 * X[:, 1],\n",
    "            np.where((X[:, 0] >= 0.5) & (X[:, 0] < 0.75), 2.5 * X[:, 1], 7.5 * X[:, 1]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate outcome\n",
    "epsilon = rng.normal(0, 1, n)\n",
    "z = outcome_mean(X) + epsilon\n",
    "y = np.where(z >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inds = np.arange(n)\n",
    "train_inds, test_inds = train_test_split(sample_inds, test_size=0.5)\n",
    "X_train = X[train_inds, :]\n",
    "X_test = X[test_inds, :]\n",
    "z_train = z[train_inds]\n",
    "z_test = z[test_inds]\n",
    "y_train = y[train_inds]\n",
    "y_test = y[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gfr = 10\n",
    "num_mcmc = 100\n",
    "bart_model = BARTModel()\n",
    "general_params = {\"num_chains\": 1, \"probit_outcome_model\": True}\n",
    "bart_model.sample(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    num_gfr=num_gfr,\n",
    "    num_mcmc=num_mcmc,\n",
    "    general_params=general_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've simulated this data, we can compare the true latent continuous outcome variable to the probit-scale predictions for a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=np.mean(bart_model.y_hat_test,axis=1), y=z_test)\n",
    "plt.axline((0, 0), slope=1, color=\"black\", linestyle=(0, (3, 3)))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On non-simulated datasets, the first thing we would evaluate is the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = np.mean(bart_model.y_hat_test,axis=1) > 0\n",
    "print(f\"Test set accuracy: {np.mean(y_test == preds_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) for every posterior sample, as well as the ROC of the posterior mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gfr = 10\n",
    "num_mcmc = 100\n",
    "fpr_list = list()\n",
    "tpr_list = list()\n",
    "threshold_list = list()\n",
    "for i in range(num_mcmc):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, bart_model.y_hat_test[:,i], pos_label=1)\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)\n",
    "    threshold_list.append(thresholds)\n",
    "probit_preds_test_mean = np.mean(bart_model.y_hat_test,axis=1)\n",
    "fpr_mean, tpr_mean, thresholds_mean = roc_curve(y_test, probit_preds_test_mean, pos_label=1)\n",
    "for i in range(num_mcmc):\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color = 'blue', linestyle='solid', linewidth = 0.9)\n",
    "plt.plot(fpr_mean, tpr_mean, color = 'black', linestyle='dashed', linewidth = 1.75)\n",
    "plt.axline((0, 0), slope=1, color=\"red\", linestyle='dashed', linewidth=1.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
